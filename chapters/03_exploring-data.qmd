# Exploring Data

```{r, echo = FALSE}
banknotes = read.csv("data/banknotes.csv")
```

Now that you have a solid foundation in the basic functions and data structures
of R, you can move on to its most popular application: data analysis. In this
chapter, you'll learn how to efficiently explore and summarize data with
visualizations and statistics. Along the way, you'll also learn how to use
apply functions, which are essential to fluency in R.

#### Learning Objectives {-}

* Describe when to use `[` versus `[[`
* Index data frames to get specific rows, columns, or subsets
* Install and load packages
* Describe the grammar of graphics
* Make a plot
* Save a plot to an image file
* Call a function repeatedly with `sapply` or `lapply`
* Split data into groups and apply a function to each


## Indexing Data Frames

This section explains how to get and set data in a data frame, expanding on the
indexing techniques you learned in @sec-indexing. Under the hood, every data
frame is a list, so first you'll learn about indexing lists.


### Indexing Lists

Lists are a **container** for other types of R objects. When you select an
element from a list, you can either keep the container (the list) or discard
it. The indexing operator `[` almost always keeps containers.

As an example, let's get some elements from a small list:

```{r}
x = list(first = c(1, 2, 3), second = sin, third = c("hi", "hello"))
y = x[c(1, 3)]
y
class(y)
```

The result is still a list. Even if we get just one element, the result of
indexing a list with `[` is a list:

```{r}
class(x[1])
```

Sometimes this will be exactly what we want. But what if we want to get the
first element of `x` so that we can use it in a vectorized function? Or in a
function that only accepts numeric arguments? We need to somehow get the
element and discard the container.

The solution to this problem is the **extraction operator** `[[`, which is also
called the double square bracket operator. The extraction operator is the
primary way to get and set elements of lists and other containers.

Unlike the indexing operator `[`, the extraction operator always discards the
container:

```{r}
x[[1]]
class(x[[1]])
```

The tradeoff is that the extraction operator can only get or set one element at
a time. Note that the element can be a vector, as above. Because it can only
get or set one element at a time, the extraction operator can only index by
position or name. Blank and logical indexes are not allowed.

The final difference between the index operator `[` and the extraction operator
`[[` has to do with how they handle invalid indexes. The index operator `[`
returns `NA` for invalid vector elements, and `NULL` for invalid list elements:

```{r}
c(1, 2)[10]
x[10]
```

On the other hand, the extraction operator `[[` raises an error for invalid
elements:

```{r, error = TRUE}
x[[10]]
```

The indexing operator `[` and the extraction operator `[[` both work with any
data structure that has elements. However, you'll generally use the indexing
operator `[` to index vectors, and the extraction operator `[[` to index
containers (such as lists).


### Two-dimensional Indexing

For two-dimensional objects, like matrices and data frames, you can pass the
indexing operator `[` or the extraction operator `[[` a separate index for each
dimension. The rows come first:

```
DATA[ROWS, COLUMNS]
```

For instance, let's get the first 3 rows and all columns of the banknotes data:

```{r}
banknotes[1:3, ]
```

As we saw in @sec-all-elements, leaving an index blank means all elements.

As another example, let's get the 3rd and 5th row, and the 2nd and 4th column:

```{r}
banknotes[c(3, 5), c(2, 4)]
```

Mixing several different ways of indexing is allowed. So for example, we can
get the same above, but use column names instead of positions:

```{r}
banknotes[c(3, 5), c("currency_name", "name")]
```

For data frames, it's especially common to index the rows by condition and the
columns by name. For instance, let's get the `name` and `profession` columns
for all women in the banknotes data set:

```{r}
result = banknotes[banknotes$gender == "F", c("name", "profession")]
head(result)
```

Also see @sec-the-drop-parameter for a case where the `[` operator behaves in a
surprising way.


## Packages {#sec-packages}

A **package** is a collection of functions for use in R. Packages usually
include documentation, and can also contain examples, vignettes, and data sets.
Most packages are developed by members of the R community, so quality varies.
There are also a few packages that are built into R but provide extra features.
We'll use a package in @sec-data-visualization, so we're learning about them
now.

The [Comprehensive R Archive Network][cran], or CRAN, is the main place people
publish packages. As of writing, there were 18,619 packages posted to CRAN.
This number has been steadily increasing as R has grown in popularity.

[cran]: https://cran.r-project.org/

Packages span a wide variety of topics and disciplines. There are packages
related to statistics, social sciences, geography, genetics, physics, biology,
pharmacology, economics, agriculture, and more. The best way to find packages
is to search online, but the CRAN website also provides ["task
views"][cran-task-views] if you want to browse popular packages related to a
specific discipline.

[cran-task-views]: https://cran.r-project.org/web/views/

The `install.packages` function installs one or more packages from CRAN. Its
first argument is the packages to install, as a character vector.

When you run `install.packages`, R will ask you to choose which **mirror** to
download the package from. A mirror is a web server that has the same set of
files as some other server. Mirrors are used to make downloads faster and to
provide redundancy so that if a server stops working, files are still available
somewhere else. CRAN has dozens of mirrors; you should choose one that's
geographically nearby, since that usually produces the best download speeds. If
you aren't sure which mirror to choose, you can use the 0-Cloud mirror, which
attempts to automatically choose a mirror near you.

As an example, here's the code to install the remotes package:

```{r, eval = FALSE}
install.packages("remotes")
```

If you run the code above, you'll be asked to select a mirror, and then see
output that looks something like this:

```
--- Please select a CRAN mirror for use in this session ---
trying URL 'https://cloud.r-project.org/src/contrib/remotes_2.3.0.tar.gz'
Content type 'application/x-gzip' length 148405 bytes (144 KB)
==================================================
downloaded 144 KB

* installing *source* package ‘remotes’ ...
** package ‘remotes’ successfully unpacked and MD5 sums checked
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (remotes)

The downloaded source packages are in
        ‘/tmp/Rtmp8t6iGa/downloaded_packages’
```

R goes through a variety of steps to install a package, even installing other
packages that the package depends on. You can tell that a package installation
succeeded by the final line `DONE`. When a package installation fails, R prints
an error message explaining the problem instead.

Once a package is installed, it stays on your computer until you remove it or
remove R. This means you only need to install each package once. However, most
packages are periodically updated. You can reinstall a package using
`install.packages` the same way as above to get the latest version.

Alternatively, you can update all of the R packages you have installed at once
by calling the `update.packages` function. Beware that this may take a long
time if you have a lot of packages installed.

The function to remove packages is `remove.packages`. Like `install.packages`,
this function's first argument is the packages to remove, as a character
vector.

If you want to see which packages are installed, you can use the
`installed.packages` function. It does not require any arguments. It returns a
matrix with one row for each package and columns that contain a variety of
information. Here's an example:

```{r}
packages = installed.packages()
# Just print the version numbers for 10 packages.
packages[1:10, "Version"]
```

You'll see a different set of packages, since you have a different computer.

Before you can use the functions (or other resources) in an installed package,
you must load the package with the `library` function. R doesn't load packages
automatically because each package you load uses memory and may conflict with
other packages. Thus you should only load the packages you need for whatever
it is that you want to do. When you restart R, the loaded packages are cleared
and you must again load any packages you want to use.

Let's load the remotes package we installed earlier:

```{r}
library("remotes")
```

The `library` function works with or without quotes around the package name, so
you may also see people write things like `library(remotes)`. We recommend
using quotes to make it unambiguous that you are not referring to a variable.

A handful of packages print out a message when loaded, but the vast majority do
not. Thus you can assume the call to `library` was successful if nothing is
printed. If something goes wrong while loading a package, R will print out an
error message explaining the problem.

Finally, not all R packages are published to CRAN. [GitHub][gh] is another
popular place to publish R packages, especially ones that are experimental or
still in development. Unlike CRAN, GitHub is a general-purpose website for
publishing code written in any programming language, so it contains much more
than just R packages and is not specifically R-focused.

[gh]: https://github.com/

The remotes package that we just installed and loaded provides functions to
install packages from GitHub. It is generally better to install packages from
CRAN when they are available there, since the versions on CRAN tend to be more
stable and intended for a wide audience. However, if you want to install a
package from GitHub, you can learn more about the remotes package by reading
its [online documentation][remotes].

[remotes]: https://remotes.r-lib.org/



## Data Visualization {#sec-data-visualization}

There are three popular systems for creating visualizations in R:

1. The base R functions (primarily the `plot` function)
2. The lattice package
3. The ggplot2 package

These three systems are not interoperable! Consequently, it's best to choose
one to use exclusively. Compared to base R, both lattice and ggplot2 are better
at handling grouped data and generally require less code to create a
nice-looking visualization.

The ggplot2 package is so popular that there are now knockoff packages for
other data-science-oriented programming languages like Python and Julia. The
package is also part of the [Tidyverse][tidy], a popular collection of R
packages designed to work well together. Because of these advantages, we'll use
ggplot2 for visualizations in this and all future lessons.

[tidy]: https://www.tidyverse.org/

ggplot2 has detailed [documentation][ggplot2-docs] and also a
[cheatsheet][ggplot2-cheat].

[ggplot2-docs]: https://ggplot2.tidyverse.org/
[ggplot2-cheat]: https://github.com/rstudio/cheatsheets/blob/master/data-visualization-2.1.pdf

The "gg" in ggplot2 stands for **grammar of graphics**. The idea of a grammar
of graphics is that visualizations can be built up in layers. In ggplot2, the
three layers every plot must have are:

* Data
* Geometry
* Aesthetics

There are also several optional layers. Here are a few:

Layer       | Description
----------  | -----------
scales      | Title, label, and axis value settings
facets      | Side-by-side plots
guides      | Axis and legend position settings
annotations | Shapes that are not mapped to data
coordinates | Coordinate systems (Cartesian, logarithmic, polar)


As an example, let's plot the banknotes data. First, we need to load ggplot2.
As always, if this is your first time using the package, you'll have to install
it. Then you can load the package:

```{r}
# install.packages("ggplot2")
library("ggplot2")
```

What kind of plot should we make? It depends on what we want to know about the
data set. Suppose we want to understand the relationship between a banknote's
value and how long ago the person on the banknote died, as well as whether this
is affected by gender. One way to show this is to make a scatter plot.

<!-- FIXME: Move this type correction to chapter 2 -->
Before plotting, we need to make sure that the `death_year` column is numeric:

```{r}
is_blank = banknotes$death_year %in% c("", "-")
banknotes$death_year[is_blank] = NA
banknotes$death_year = as.numeric(banknotes$death_year)
```

Now we're ready to make the plot.

### Layer 1: Data

The data layer determines the data set used to make the plot. ggplot and most
other Tidyverse packages are designed for working with **tidy** data frames.
Tidy means:

1. Each observation has its own row.
2. Each feature has its own column.
3. Each value has its own cell.

Tidy data sets are convenient in general. A later lesson will cover how to make
an untidy data set tidy. Until then, we'll take it for granted that the data
sets we work with are tidy.

To set up the data layer, call the `ggplot` function on a data frame:
```{r}
ggplot(banknotes)
```

This returns a blank plot. We still need to add a few more layers.


### Layer 2: Geometry

The **geom**etry layer determines the shape or appearance of the visual
elements of the plot. In other words, the geometry layer determines what kind
of plot to make: one with points, lines, boxes, or something else.

There are many different geometries available in ggplot2. The package provides
a function for each geometry, always prefixed with `geom_`.

To add a geometry layer to the plot, choose the `geom_` function you want and
add it to the plot with the `+` operator:

```{r, error=TRUE, fig.show="hide"}
ggplot(banknotes) + geom_point()
```

This returns an error message that we're missing aesthetics `x` and `y`. We'll
learn more about aesthetics in the next section, but this error message is
especially helpful: it tells us exactly what we're missing. When you use a
geometry you're unfamiliar with, it can be helpful to run the code for just the
data and geometry layer like this, to see exactly which aesthetics need to be
set.

As we'll see later, it's possible to add multiple geometries to a plot.


### Layer 3: Aesthetics

The **aes**thetic layer determines the relationship between the data and the
geometry. Use the aesthetic layer to map features in the data to **aesthetics**
(visual elements) of the geometry.

The `aes` function creates an aesthetic layer. The syntax is:
```
aes(AESTHETIC = FEATURE, ...)
```

The names of the aesthetics depend on the geometry, but some common ones are
`x`, `y`, `color`, `fill`, `shape`, and `size`. There is more information about
and examples of aesthetic names in the documentation.

For example, we want to put `death_year` on the x-axis and `scaled_bill_value`
on the y-axis. It's best to use `scaled_bill_value` here rather than
`current_bill_value` because the different countries use different scales of
currency. For example, 1 United States Dollar is worth approximately 100
Japanese Yen. So the aesthetic layer should be:
```{r}
ggplot(banknotes) +
  geom_point() +
  aes(x = death_year, y = scaled_bill_value)
```

In the `aes` function, column names are never quoted. In older versions of
ggplot2, you must pass the aesthetic layer as the second argument of the
`ggplot` function rather than using `+` to add it to the plot. This syntax is
still widely used:
```{r}
ggplot(banknotes, aes(x = death_year, y = scaled_bill_value)) +
  geom_point()
```

#### Per-geometry Aesthetics {-}

When you add the aesthetic layer or pass it to the `ggplot` function, it
applies to the entire plot. You can also set an aesthetic layer individually
for each geometry, by passing the layer as the first argument in the `geom_`
function:
```{r}
ggplot(banknotes) +
  geom_point(aes(x = death_year, y = scaled_bill_value))
```

This is really only useful when you have multiple geometries. As an example,
let's color-code the points by gender:
```{r}
ggplot(banknotes) +
  geom_point(aes(x = death_year, y = scaled_bill_value, color = gender))
```

Now let's also add labels to each point. To do this, we need to add another
geometry:
```{r}
ggplot(banknotes) +
  geom_point() +
  aes(x = death_year, y = scaled_bill_value, color = gender, label = name) +
  geom_text()
```

Where we put the aesthetics matters:
```{r}
ggplot(banknotes) +
  geom_point() +
  aes(x = death_year, y = scaled_bill_value, label = name) +
  geom_text(aes(color = gender))
```


#### Constant Aesthetics {-}

If you want to set an aesthetic to a constant value, rather than one that's
data dependent, do so in the geometry layer rather than the aesthetic layer.
For instance, suppose use point shape rather than color to indicate gender, and
we want to make all of the points blue:

```{r}
ggplot(banknotes) +
  geom_point(color = "blue") +
  aes(x = death_year, y = scaled_bill_value, shape = gender)
```

If you set an aesthetic to a constant value inside of the aesthetic layer, the
results you get might not be what you expect:

```{r}
ggplot(banknotes) +
  geom_point() +
  aes(x = death_year, y = scaled_bill_value, shape = gender, color = "blue")
```


### Layer 4: Scales

The scales layer controls the title, axis labels, and axis scales of the plot.
Most of the functions in the scales layer are prefixed with `scale_`, but not
all of them.

The `labs` function is especially important, because it's used to set the title
and axis labels:
```{r}
ggplot(banknotes) +
  geom_point() +
  aes(x = death_year, y = scaled_bill_value, shape = gender) +
  labs(x = "Death Year", y = "Scaled Bill Value",
    title = "Does death year affect bill value?", shape = "Gender")
```


### Saving Plots

In ggplot2, use the `ggsave` function to save the most recent plot you created:

```{r, eval=FALSE}
ggsave("banknote_scatter.png")
```

The file format is selected automatically based on the extension. Common
formats are PNG and PDF.

#### The Plot Device {-}

You can also save a plot with one of R's "plot device" functions. The steps
are:

1. Call a plot device function: `png`, `jpeg`, `pdf`, `bmp`, `tiff`, or `svg`.
2. Run your code to make the plot.
3. Call `dev.off` to indicate that you're done plotting.

This strategy works with any of R's graphics systems (not just ggplot2).

Here's an example:
```{r, eval=FALSE}
# Run these lines in the console, not the notebook!
jpeg("banknote_scatter.jpeg")
ggplot(banknotes) +
  geom_point() +
  aes(x = death_year, y = scaled_bill_value, shape = gender) +
  labs(x = "Death Year", y = "Scaled Bill Value",
    title = "Does death year affect bill value?", shape = "Gender")
dev.off()
```


### Example: Bar Plot

Now suppose you want to plot the number of banknotes with people from each
profession in the banknotes data set. A bar plot is in an appropriate way to
represent this visually.

The geometry for a bar plot is `geom_bar`. Since bar plots are mainly used to
display frequencies, the `geom_bar` function automatically computes frequencies
when given mapped to a categorical feature.

We can also use a fill color to further breakdown the bars by gender. Here's
the code to make the bar plot:
```{r}
ggplot(banknotes) +
  aes(x = profession, fill = gender) +
  geom_bar(position = "dodge")
```

The setting `position = "dodge"` instructs `geom_bar` to put the bars
side-by-side rather than stacking them.

In some cases, you may want to make a bar plot with frequencies you've already
computed. To prevent `geom_bar` from computing frequencies automatically, set
`stat = "identity"`.


### Visualization Design

Designing high-quality visualizations goes beyond just mastering which R
functions to call. You also need to think carefully about what kind of data you
have and what message you want to convey. This section provides a few
guidelines.

The first step in data visualization is choosing an appropriate kind of plot.
Here are some suggestions (not rules):

| Feature 1   | Feature 2   | Plot
| :---------- |:----------- |:----
| categorical |             | bar, dot
| categorical | categorical | bar, dot, mosaic
| numerical   |             | box, density, histogram
| numerical   | categorical | box, density, ridge
| numerical   | numerical   | line, scatter, smooth scatter

If you want to add a:

* 3rd numerical feature, use it to change point/line sizes.
* 3rd categorical feature, use it to change point/line styles.
* 4th categorical feature, use side-by-side plots.

Once you've selected a plot, here are some rules you should almost always
follow:

* Always add a title and axis labels. These should be in plain English, not
  variable names!

* Specify units after the axis label if the axis has units. For instance,
  "Height (ft)".

* Don't forget that many people are colorblind! Also, plots are often printed
  in black and white. Use point and line styles to distinguish groups; color is
  optional.

* Add a legend whenever you've used more than one point or line style.

* Always write a few sentences explaining what the plot reveals. Don't
  describe the plot, because the reader can just look at it. Instead,
  explain what they can learn from the plot and point out important details
  that are easily overlooked.

* Sometimes points get plotted on top of each other. This is called
  _overplotting_. Plots with a lot of overplotting can be hard to read and can
  even misrepresent the data by hiding how many points are present. Use a
  two-dimensional density plot or jitter the points to deal with overplotting.

* For side-by-side plots, use the same axis scales for both plots so that
  comparing them is not deceptive.

Visualization design is a deep topic, and whole books have been written about
it. One resource where you can learn more is DataLab's [Principles of Data
Visualization Workshop Reader][dataviz].

[dataviz]: https://ucdavisdatalab.github.io/workshop_data_viz_principles/

Apply Functions
---------------

@sec-vectorization introduced vectorization, a convenient and efficient way to
compute multiple results. That section also mentioned that some of R's
functions---the ones that summarize or aggregate data---are not vectorized.

The `class` function is an example of a function that's not vectorized. If we
call the `class` function on the banknotes data set, we get just one result for
the data set as a whole:

```{r}
class(banknotes)
```

What if we want to get the class of each column? We can get the class for a
single column by selecting the column with `$`, the dollar sign operator:

```{r}
class(banknotes$currency_code)
```

But what if we want the classes for all the columns? We could write a call to
`class` for each column, but that would be tedious. When you're working with a
programming language, you should try to avoid tedium; there's usually a better,
more automated way.

@sec-lists pointed out that data frames are technically lists, where each
column is one element. With that in mind, what we need here is a line of code
that calls `class` on each element of the data frame. The idea is similar to
vectorization, but since we have a list and a non-vectorized function, we have
to do a bit more than just call `class(banknotes)`.

The `lapply` function calls, or _applies_, a function on each element of a list
or vector. The syntax is:

```
lapply(X, FUN, ...)
```

The function `FUN` is called once for each element of `X`, with the element as
the first argument. The `...` is for additional arguments to `FUN`, which are
held constant across all the elements.

Let's try this out with the banknotes data and the `class` function:

```{r}
lapply(banknotes, class)
```

The result is similar to if the `class` function was vectorized. In fact, if we
use a vector and a vectorized function with `lapply`, the result is nearly
identical to the result from vectorization:

```{r}
x = c(1, 2, pi)

sin(x)

lapply(x, sin)
```

The only difference is that the result from `lapply` is a list. In fact, the
`lapply` function always returns a list with one element for each element of
the input data. The "l" in `lapply` stands for "list".

The `lapply` function is one member of a family of functions called _apply
functions_. All of the apply functions provide ways to apply a function
repeatedly to different parts of a data structure. We'll meet a few more apply
functions soon.

When you have a choice between using vectorization or an apply function, you
should always choose vectorization. Vectorization is clearer---compare the two
lines of code above---and it's also significantly more efficient. In fact,
vectorization is the most efficient way to call a function repeatedly in R.

As we saw with the `class` function, there are some situations where
vectorization is not possible. That's when you should think about using an
apply function.

### The `sapply` Function

The related `sapply` function calls a function on each element of a list or
vector, and simplifies the result. That last part is the crucial difference
compared to `lapply`. When results from the calls all have the same type and
length, `sapply` returns a vector or matrix instead of a list. When the results
have different types or lengths, the result is the same as for `lapply`. The
"s" in `sapply` stands for "simplify".

For instance, if we use `sapply` to find the classes of the columns in the
banknotes data, we get a character vector:

```{r}
sapply(banknotes, class)
```

Likewise, if we use `sapply` to compute the `sin` values, we get a numeric
vector, the same as from vectorization:

```{r}
sapply(x, sin)
```

In spite of that, vectorization is still more efficient than `sapply`, so use
vectorization instead when possible.

Apply functions are incredibly useful for summarizing data. For example,
suppose we want to compute the frequencies for all of the columns in the
banknotes data set that aren't numeric.

First, we need to identify the columns. One way to do this is with the
`is.numeric` function. Despite the name, this function actually tests whether
its argument is a real number, not whether it its argument is a numeric vector.
In other words, it also returns true for integer values. We can use `sapply` to
apply this function to all of the columns in the banknotes data set:

```{r}
is_not_number = !sapply(banknotes, is.numeric)
is_not_number
```

Is it worth using R code to identify the non-numeric columns? Since there are
only `r ncol(banknotes)` columns in the banknotes data set, maybe not. But if
the data set was larger, with say 100 columns, it definitely would be.

In general, it's a good habit to use R to do things rather than do them
manually. You'll get more practice programming, and your code will be more
flexible if you want to adapt it to other data sets.

Now that we know which columns are non-numeric, we can use the `table` function
to compute frequencies. We only want to compute frequencies for those columns,
so we need to subset the data:

```{r}
lapply(banknotes[, is_not_number], table)
```

We use `lapply` rather than `sapply` for this step because the table for each
column will have a different length (but try `sapply` and see what happens!).


### The Split-Apply Pattern

In a data set with categorical features, it's often useful to compute something
for each category. The `lapply` and `sapply` functions can compute something
for each element of a data structure, but categories are not necessarily
elements.

```{r, echo = FALSE}
n_countries = length(unique(banknotes$country))
```

For example, the banknotes data set has `r n_countries` different categories in
the `country` column. If we want all of the rows for one country, one way to
get them is by indexing:

```{r}
usa = banknotes[banknotes$country == "United States", ]
head(usa)
```

To get all `r n_countries` countries separately, we'd have to do this
`r n_countries` times. If we want to compute something for each country, say
the mean of the `first_appearance_year` column, we also have to repeat that
computation `r n_countries` times. Here's what it would look like for just the
United States:

```{r}
mean(usa$first_appearance_year)
```

If the categories were elements, we could avoid writing code to index each
category, and just use the `sapply` (or `lapply`) function to apply the `mean`
function to each.

The `split` function splits a vector or data frame into groups based on a
vector of categories. The first argument to `split` is the data, and the
second argument is a congruent vector of categories.

We can use `split` to elegantly compute means of `first_appearance_year` broken
down by country. First, we split the data by country. Since we only want to
compute on the `first_appearance_year` column, we only split that column:

```{r}
by_country = split(banknotes$first_appearance_year, banknotes$country)
class(by_country)
names(by_country)
```
 
The result from `split` is a list with one element for each category. The
individual elements contain pieces of the original `first_appearance_year`
column:

```{r}
head(by_country$Mexico)
```

Since the categories are elements in the split data, now we can use `sapply`
the same way we did in previous examples:

```{r}
sapply(by_country, mean)
```

This two-step process is an R idiom called the _split-apply pattern_. First you
use `split` to convert categories into list elements, then you use an apply
function to compute something on each category. Any time you want to compute
results by category, you should think of this pattern.

The split-apply pattern is so useful that R provides the `tapply` function as a
shortcut. The `tapply` function is equivalent to calling `split` and then
`sapply`. Like `split`, the first argument is the data and the second argument
is a congruent vector of categories. The third argument is a function to apply,
like the function argument in `sapply`.

We can use `tapply` to compute the `first_appearance_year` means by `country`
for the banknotes data set:

```{r}
tapply(banknotes$first_appearance_year, banknotes$country, mean)
```

Notice that the result is identical to the one we computed before.

The "t" in `tapply` stands for "table", because the `tapply` function is a
generalization of the `table` function. If you use `length` as the third
argument to `tapply`, you get the same results as you would from using the
`table` function on the category vector.

The `aggregate` function is closely related to `tapply`. It computes the same
results, but organizes them into a data frame with one row for each category.
In some cases, this format is more convenient. The arguments are the same,
except that the second argument must be a list or data frame rather than a
vector. 

As an example, here's the result of using `aggregate` to compute the
`first_appearance_year` means:

```{r}
aggregate(banknotes$first_appearance_year, list(banknotes$country), mean)
```

The `lapply`, `sapply`, and `tapply` functions are the three most important
functions in the family of apply functions, but there are many more. You can
learn more about all of R's apply functions by reading [this StackOverflow
post][apply].

[apply]: https://stackoverflow.com/a/7141669


Exercises
---------

### Exercise

Compute the number of banknotes that feature a person who died before 1900.

### Exercise

Compute the range of `first_appearance_year` for each country.

### Exercise

1. Compute the set of banknotes with people who died in this century.

   _Hint: be careful of missing values in the `death_year` column!_

```{r}
is21 = !is.na(banknotes$death_year) & banknotes$death_year >= 2000
people21 = banknotes[is21, ]
```

2. Use ggplot2's `geom_segment` function to create a plot which shows the
   timespan between death year and first appearance as a horizontal segment,
   for each banknote in the result from step 1. Put the name of each person on
   the y-axis. Color code the segments by gender.

   _Hint: You can make the plot more visually appealing if you first sort the
   data by death year. You can use the `order` function to get indexes that
   will sort the rows of a data frame according to some column._
